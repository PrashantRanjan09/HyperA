{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_json('multinli_1.0/multinli_1.0_train.jsonl',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>promptID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>government</td>\n",
       "      <td>neutral</td>\n",
       "      <td>31193n</td>\n",
       "      <td>31193</td>\n",
       "      <td>Conceptually cream skimming has two basic dime...</td>\n",
       "      <td>( ( Conceptually ( cream skimming ) ) ( ( has ...</td>\n",
       "      <td>(ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...</td>\n",
       "      <td>Product and geography are what make cream skim...</td>\n",
       "      <td>( ( ( Product and ) geography ) ( ( are ( what...</td>\n",
       "      <td>(ROOT (S (NP (NN Product) (CC and) (NN geograp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>entailment</td>\n",
       "      <td>101457e</td>\n",
       "      <td>101457</td>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>( you ( ( know ( during ( ( ( the season ) and...</td>\n",
       "      <td>(ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>( You ( ( ( ( lose ( the things ) ) ( to ( the...</td>\n",
       "      <td>(ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>134793e</td>\n",
       "      <td>134793</td>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>( ( One ( of ( our number ) ) ) ( ( will ( ( (...</td>\n",
       "      <td>(ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>( ( ( A member ) ( of ( my team ) ) ) ( ( will...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>fiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>37397e</td>\n",
       "      <td>37397</td>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...</td>\n",
       "      <td>(ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>( ( This information ) ( ( belongs ( to them )...</td>\n",
       "      <td>(ROOT (S (NP (DT This) (NN information)) (VP (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>telephone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>50563n</td>\n",
       "      <td>50563</td>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>( yeah ( i ( ( tell you ) ( what ( ( though ( ...</td>\n",
       "      <td>(ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...</td>\n",
       "      <td>The tennis shoes have a range of prices.</td>\n",
       "      <td>( ( The ( tennis shoes ) ) ( ( have ( ( a rang...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotator_labels       genre  gold_label   pairID  promptID  \\\n",
       "0        [neutral]  government     neutral   31193n     31193   \n",
       "1     [entailment]   telephone  entailment  101457e    101457   \n",
       "2     [entailment]     fiction  entailment  134793e    134793   \n",
       "3     [entailment]     fiction  entailment   37397e     37397   \n",
       "4        [neutral]   telephone     neutral   50563n     50563   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  Conceptually cream skimming has two basic dime...   \n",
       "1  you know during the season and i guess at at y...   \n",
       "2  One of our number will carry out your instruct...   \n",
       "3  How do you know? All this is their information...   \n",
       "4  yeah i tell you what though if you go price so...   \n",
       "\n",
       "                              sentence1_binary_parse  \\\n",
       "0  ( ( Conceptually ( cream skimming ) ) ( ( has ...   \n",
       "1  ( you ( ( know ( during ( ( ( the season ) and...   \n",
       "2  ( ( One ( of ( our number ) ) ) ( ( will ( ( (...   \n",
       "3  ( ( How ( ( ( do you ) know ) ? ) ) ( ( All th...   \n",
       "4  ( yeah ( i ( ( tell you ) ( what ( ( though ( ...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (JJ Conceptually) (NN cream) (NN ...   \n",
       "1  (ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN...   \n",
       "2  (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PR...   \n",
       "3  (ROOT (S (SBARQ (WHADVP (WRB How)) (SQ (VBP do...   \n",
       "4  (ROOT (S (VP (VB yeah) (S (NP (FW i)) (VP (VB ...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  Product and geography are what make cream skim...   \n",
       "1  You lose the things to the following level if ...   \n",
       "2  A member of my team will execute your orders w...   \n",
       "3                  This information belongs to them.   \n",
       "4           The tennis shoes have a range of prices.   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( ( ( Product and ) geography ) ( ( are ( what...   \n",
       "1  ( You ( ( ( ( lose ( the things ) ) ( to ( the...   \n",
       "2  ( ( ( A member ) ( of ( my team ) ) ) ( ( will...   \n",
       "3  ( ( This information ) ( ( belongs ( to them )...   \n",
       "4  ( ( The ( tennis shoes ) ) ( ( have ( ( a rang...   \n",
       "\n",
       "                                     sentence2_parse  \n",
       "0  (ROOT (S (NP (NN Product) (CC and) (NN geograp...  \n",
       "1  (ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT...  \n",
       "2  (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN o...  \n",
       "3  (ROOT (S (NP (DT This) (NN information)) (VP (...  \n",
       "4  (ROOT (S (NP (DT The) (NN tennis) (NNS shoes))...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contradiction    130903\n",
       "neutral          130900\n",
       "entailment       130899\n",
       "Name: gold_label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gold_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Conceptually cream skimming has two basic dime...\n",
       "1         you know during the season and i guess at at y...\n",
       "2         One of our number will carry out your instruct...\n",
       "3         How do you know? All this is their information...\n",
       "4         yeah i tell you what though if you go price so...\n",
       "5         my walkman broke so i'm upset now i just have ...\n",
       "6         But a few Christian mosaics survive above the ...\n",
       "7          (Read  for Slate 's take on Jackson's findings.)\n",
       "8                                        Gays and lesbians.\n",
       "9         At the end of Rue des Francs-Bourgeois is what...\n",
       "10        I burst through a set of cabin doors, and fell...\n",
       "11                             Fun for adults and children.\n",
       "12        It's not that the questions they asked weren't...\n",
       "13        Thebes held onto power until the 12th Dynasty,...\n",
       "14        I don't mean to be glib about your concerns, b...\n",
       "15                                Issues in Data Synthesis.\n",
       "16                     well you see that on television also\n",
       "17        Vrenna and I both fought him and he nearly too...\n",
       "18        This analysis pooled estimates from these two ...\n",
       "19                          He turned and smiled at Vrenna.\n",
       "20        We sought to identify practices that were comm...\n",
       "21                                  The other men shuffled.\n",
       "22        States must show reasonable progress in their ...\n",
       "23                          well it's been very interesting\n",
       "24                 He started slowly back to the bunkhouse.\n",
       "25        and it's it's quite a bit i think six somethin...\n",
       "26        They're made from a secret recipe handed down ...\n",
       "27                         yeah well you're a student right\n",
       "28        it really is i heard something that their supp...\n",
       "29        Postal Service were to reduce delivery frequency.\n",
       "                                ...                        \n",
       "392672    The non-stop lift takes 55 seconds from ground...\n",
       "392673    Got you a couple of blooded hosses an' a good ...\n",
       "392674    Breasts have lost much of their mythological a...\n",
       "392675            Which of you is the strongest? asked Jon.\n",
       "392676    and losing it in the competitive market and a ...\n",
       "392677    The initial approach route, along the A593 fro...\n",
       "392678    of it being a little bit too low and i my bath...\n",
       "392679    she likes to sew and do crafts and things like...\n",
       "392680    The river Canche flows right through the cente...\n",
       "392681    Uphill to the south of the park lies the Agora...\n",
       "392682    In additional to internal communications, mana...\n",
       "392683    Suffice to say it has no true heat, but does s...\n",
       "392684    And the teens in that one were so much meaner ...\n",
       "392685    no no i'm in Detroit or not Detroit i'm in uh ...\n",
       "392686    Dolly Parton's breast implants against Mark Mc...\n",
       "392687    The New Radicals sound like Todd Rundgren has ...\n",
       "392688    Its majesty is most notable in the balconies o...\n",
       "392689    but they they've got so much young raw talent ...\n",
       "392690    Concerns about the reliability of value estima...\n",
       "392691    If you would like to scale the highest heights...\n",
       "392692    Many responses pivoted on the amusing contrast...\n",
       "392693    Four monasteries developed as a source of prot...\n",
       "392694             There was nothing like that emotion now.\n",
       "392695                                     Drawing a blank?\n",
       "392696    Legacy, predicting that he will keep custody o...\n",
       "392697      Clearly, California can - and must - do better.\n",
       "392698    It was once regarded as the most beautiful str...\n",
       "392699    Houseboats are a beautifully preserved traditi...\n",
       "392700    Obituaries fondly recalled his on-air debates ...\n",
       "392701    in that other you know uh that i should do it ...\n",
       "Name: sentence1, Length: 392702, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentence1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Product and geography are what make cream skim...\n",
       "1         You lose the things to the following level if ...\n",
       "2         A member of my team will execute your orders w...\n",
       "3                         This information belongs to them.\n",
       "4                  The tennis shoes have a range of prices.\n",
       "5         I'm upset that my walkman broke and now I have...\n",
       "6         Most of the Christian mosaics were destroyed b...\n",
       "7               Slate had an opinion on Jackson's findings.\n",
       "8                                            Heterosexuals.\n",
       "9         Place des Vosges is constructed entirely of gr...\n",
       "10                 I burst through the doors and fell down.\n",
       "11                                   Fun for only children.\n",
       "12        All of the questions were interesting accordin...\n",
       "13        The capital near Memphis lasted only half a ce...\n",
       "14        I am concerned more about your issues than the...\n",
       "15                              Problems in data synthesis.\n",
       "16                 You can see that on television, as well.\n",
       "17          Neither Vrenna nor myself have ever fought him.\n",
       "18        The analysis proves that there is no link betw...\n",
       "19        He smiled at Vrenna who was walking slowly beh...\n",
       "20        We want to identify practices commonly used by...\n",
       "21                      The other men were shuffled around.\n",
       "22        Itis not necessary for there to be any improve...\n",
       "23                             It has been very intriguing.\n",
       "24                     He returned slowly to the bunkhouse.\n",
       "25          I do not know exactly where the local taxes go.\n",
       "26        The recipe passed down from Mallorcan ancestor...\n",
       "27                   Well you're a mechanics student right?\n",
       "28        It's unfortunate that nobody is organizing a c...\n",
       "29        The postal service could deliver less frequently.\n",
       "                                ...                        \n",
       "392672    The lift was designed to offer the spectacular...\n",
       "392673                            He gave him a money belt.\n",
       "392674           People are not excited by breasts anymore.\n",
       "392675    The man asked for directions to the nearest ex...\n",
       "392676    I really wonder how America has lost its compe...\n",
       "392677    The route coming from Ambleside along the A593...\n",
       "392678    I am wearing a very modest swimsuit that i bou...\n",
       "392679    She used to work a fulltime job, before she de...\n",
       "392680                  The town has no water source in it.\n",
       "392681    The hill to get to Agora is not steep and wort...\n",
       "392682    Communication should be conducted over email a...\n",
       "392683                            It has lots of true heat.\n",
       "392684                   Bobbie Rydell is a known teenager.\n",
       "392685     I'm no longer in Detroit, I moved to California.\n",
       "392686    Dolly Parton's breast implants contrasting Mar...\n",
       "392687           The New Radicals sound like Todd Rundgren.\n",
       "392688    The latticework in the Royal Harem is clumsy a...\n",
       "392689    They will be going to a camp to learn how to p...\n",
       "392690            Research on bias can be biased as well.  \n",
       "392691    Some lessons and advice are a good idea if you...\n",
       "392692    There was no visible difference between the up...\n",
       "392693    Only three monasteries still exist here, and a...\n",
       "392694              There are few emotions that come close.\n",
       "392695                          You can be drawing a blank.\n",
       "392696    His children love their father and it's a grea...\n",
       "392697                     California cannot do any better.\n",
       "392698    So many of the original buildings had been rep...\n",
       "392699    The tradition of houseboats originated while t...\n",
       "392700    The obituaries were beautiful and written in k...\n",
       "392701    My husband has been so overworked lately that ...\n",
       "Name: sentence2, Length: 392702, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentence2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_df = pd.read_csv('glove.6B/glove.6B.300d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove = {key: val.values for key, val in glove_df.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_puncts = {'`': \"'\", '′': \"'\", '“':'\"', '”': '\"', '‘': \"'\"}\n",
    "\n",
    "strip_chars = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '[', ']', '>', '=', '+', '\\\\', '•',  '~', '@', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "puncts = ['!', '?', '$', '&', '/', '%', '#', '*','£']\n",
    "\n",
    "def clean_str(x):\n",
    "    x = str(x)\n",
    "    \n",
    "    x = x.lower()\n",
    "    \n",
    "    x = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", x)\n",
    "    \n",
    "    for k, v in replace_puncts.items():\n",
    "        x = x.replace(k, f' {v} ')\n",
    "        \n",
    "    for punct in strip_chars:\n",
    "        x = x.replace(punct, ' ') \n",
    "    \n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "        \n",
    "    x = x.replace(\" '\", \" \")\n",
    "    x = x.replace(\"' \", \" \")\n",
    "        \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_sentence1'] = data['sentence1'].apply(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_sentence2'] = data['sentence2'].apply(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /anaconda3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.6/site-packages (from keras) (1.0.6)\n",
      "Requirement already satisfied: scipy>=0.14 in /anaconda3/lib/python3.6/site-packages (from keras) (1.2.0)\n",
      "Requirement already satisfied: pyyaml in /anaconda3/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /anaconda3/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /anaconda3/lib/python3.6/site-packages (from keras) (1.15.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.6/site-packages (from keras) (1.0.5)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.6/site-packages (from keras) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "for i in range(len(data)):\n",
    "    sent1_words = data[\"clean_sentence1\"][i].split()\n",
    "    sent2_words = data[\"clean_sentence2\"][i].split()\n",
    "    X.append(sent1_words+sent2_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sent1_sent2'] = [' '.join(map(str, l)) for l in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data[\"gold_label_encoded\"] = le.fit_transform(data[\"gold_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = data[\"gold_label_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('conceptually cream skimming has two basic dimensions product and geography product and geography are what make cream skimming work',\n",
       " 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data['sent1_sent2'][0],y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size 39271\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000 \n",
    "sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n",
    "tokenizer.fit_on_texts(data['sent1_sent2'].values)\n",
    "X = tokenizer.texts_to_sequences(data['sent1_sent2'].values)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "\n",
    "\n",
    "y = pd.get_dummies(data['gold_label']).values\n",
    "\n",
    "# lets keep a couple of thousand samples back as a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(\"test set size \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000 \n",
    "sequence_length = 100\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>', filters=' ')\n",
    "tokenizer.fit_on_texts(data['sent1_sent2'].values)\n",
    "X_Hypothesis= tokenizer.texts_to_sequences(data['clean_sentence1'].values)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X_Hypothesis = pad_sequences(X_Hypothesis, sequence_length,padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_premise= tokenizer.texts_to_sequences(data['clean_sentence2'].values)\n",
    "\n",
    "# we then pad the sequences so they're all the same length (sequence_length)\n",
    "X_premise = pad_sequences(X_premise, sequence_length,padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = []\n",
    "for i in range(len(X_Hypothesis)):\n",
    "    Input.append((X_Hypothesis[i],X_premise[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = np.array(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data[\"gold_label_encoded\"] = le.fit_transform(data[\"gold_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"gold_label_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''required y - 0,1,2\n",
    "to get in original label form:\n",
    "do le.inverse_transform\n",
    "\n",
    "'''\n",
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_training=[]\n",
    "for i in range(len(X_Hypothesis)):\n",
    "    data_for_training.append((Input[i],y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data you need\n",
    "\n",
    "data_for_training = np.array(data_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('glove.6B', 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70666 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20001\n"
     ]
    }
   ],
   "source": [
    "num_words = min(max_features, len(word_index)) + 1\n",
    "print(num_words)\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "# first create a matrix of zeros, this is our embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# for each word in out tokenizer lets try to find that work in our w2v model\n",
    "for word, i in word_index.items():\n",
    "    if i > max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # we found the word - add that words vector to the matrix\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # doesn't exist, assign a random vector\n",
    "        embedding_matrix[i] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.40321148e+00, -1.72426198e+00,  1.13416828e+00, -8.45881547e-01,\n",
       "       -1.80816954e+00, -4.85328715e-02,  1.69591031e+00,  4.17371559e-01,\n",
       "       -2.28151247e-01, -6.14572996e-01,  2.60956198e-01, -3.90093246e-01,\n",
       "       -2.66945902e-02,  2.71881494e-01,  1.97920797e-01, -1.21738604e+00,\n",
       "        2.02166286e+00, -1.45422490e+00, -2.93138143e+00,  1.49728355e+00,\n",
       "       -1.19955804e+00, -1.46681275e+00,  1.18539437e+00, -1.63293255e+00,\n",
       "        1.16645019e+00, -7.97348764e-01, -1.22255764e+00, -4.74676155e-02,\n",
       "        4.06969275e-01, -1.41974262e-01,  1.03432055e+00,  7.70061024e-01,\n",
       "        9.29975058e-01, -2.36867706e-01,  2.91366647e-01, -5.74054295e-01,\n",
       "        1.57131245e+00, -1.02976898e+00, -2.52526656e-01,  1.73607447e+00,\n",
       "        1.47474513e+00,  1.55667301e+00,  1.66977495e+00, -2.02412609e-01,\n",
       "       -1.63701807e+00, -1.03146287e+00,  3.63289088e-01,  1.07893125e+00,\n",
       "        1.39084959e+00,  1.74903851e-01,  9.29371221e-01, -2.23387312e-01,\n",
       "        9.05979803e-01, -2.60390611e-01,  5.92693209e-01,  9.28106592e-02,\n",
       "       -3.57854392e-01,  3.93693607e-01, -7.65931493e-01,  6.66083319e-01,\n",
       "        2.57398664e-03,  4.78309410e-01,  7.26739348e-02,  1.83988416e-01,\n",
       "       -1.54571606e-01, -6.86880204e-01,  7.07313484e-01,  1.88535568e-02,\n",
       "        5.54494422e-01,  3.47145466e-01,  2.91903951e+00, -4.92581074e-02,\n",
       "       -6.08205100e-01,  1.64650154e+00, -1.78951528e+00, -2.05529849e+00,\n",
       "       -1.06147867e+00, -7.99628003e-01,  1.89826087e+00,  3.63636982e-01,\n",
       "       -1.02132319e+00, -1.84993472e-01,  7.55493007e-01,  2.27522652e-01,\n",
       "       -1.73257729e+00, -1.26038460e+00, -5.00447470e-01,  8.96773961e-01,\n",
       "        2.09855418e-03, -1.71556396e+00,  9.42369154e-01,  7.61816566e-01,\n",
       "       -1.22984840e-01,  5.78062765e-01,  1.59423852e-01,  1.20856189e+00,\n",
       "       -9.32066383e-01, -1.29597853e+00, -5.93890230e-01, -9.58897738e-01,\n",
       "       -8.96290906e-01, -6.14740928e-01, -4.15738460e-01, -1.44641615e+00,\n",
       "       -4.68345138e-01, -8.56537103e-01,  1.74267559e+00, -1.81969508e+00,\n",
       "       -1.53914650e+00, -5.65943502e-01,  5.30356718e-01,  2.14689912e+00,\n",
       "       -8.89674709e-01,  1.48742947e+00, -1.43058098e+00,  1.63367927e+00,\n",
       "        3.67849049e-01,  6.47474900e-01,  5.49069198e-01,  1.88169102e+00,\n",
       "       -1.11709301e+00, -2.19826573e+00, -2.78291728e-01,  6.08014000e-01,\n",
       "       -9.58168804e-01,  5.17793160e-01,  9.80799721e-01,  1.28567595e+00,\n",
       "        1.18808710e+00,  1.74508640e+00,  4.04873083e-01,  4.50616273e-01,\n",
       "        1.47808904e+00, -1.65964501e+00, -6.85703445e-01, -8.46516358e-02,\n",
       "       -1.75402377e-01,  1.15409383e+00, -2.88317327e-01, -1.03031433e-01,\n",
       "       -2.22233948e+00,  9.89118848e-01, -6.34628535e-01, -1.65992730e+00,\n",
       "        2.81205510e+00, -6.59478716e-01, -6.85075156e-01,  1.86918107e+00,\n",
       "       -5.66839569e-01,  6.32477777e-02,  6.30835395e-02,  4.83280709e-01,\n",
       "       -8.65047674e-01, -6.19340913e-01,  6.59736711e-01,  7.46608719e-01,\n",
       "        9.02873165e-01,  2.24866477e-01,  1.45009861e+00,  2.88479511e-01,\n",
       "       -4.51987991e-01, -4.96431102e-01,  1.41110680e+00, -7.33788256e-01,\n",
       "       -2.79910362e-01,  1.12878576e+00,  1.43964568e+00, -1.17836997e+00,\n",
       "       -5.56175874e-01,  6.77281879e-02,  1.61730814e+00,  1.45189137e+00,\n",
       "        4.44719000e-01,  1.58771009e-02, -1.54220285e+00,  1.22675346e-01,\n",
       "        1.95196710e-01, -6.68230395e-01,  3.41328357e-03,  7.24802083e-01,\n",
       "        2.20760883e-01,  6.13286472e-01, -7.37017641e-01,  1.53193212e+00,\n",
       "       -5.96241143e-01,  1.25513874e+00, -1.45108416e+00, -6.17844716e-02,\n",
       "       -1.21546936e+00, -1.84259164e-01,  1.57298192e+00, -2.72071344e-01,\n",
       "       -1.83560102e+00,  9.09549132e-01,  5.39301225e-01,  8.08897153e-01,\n",
       "       -2.43438351e-01, -1.97232004e-01, -4.09359834e-01,  9.15836878e-01,\n",
       "       -2.00497234e+00,  5.64898052e-01,  1.28324862e-01, -6.87591863e-01,\n",
       "        2.90923690e-02, -1.35080010e-01, -2.07719158e-01, -4.61324409e-01,\n",
       "        6.96831582e-01, -2.36658902e+00,  1.92189639e+00, -3.92957223e-01,\n",
       "       -4.57895403e-02,  1.28087896e+00,  1.83382444e-01, -2.09687013e+00,\n",
       "        1.85921990e+00, -9.56701739e-02, -8.99736017e-01, -4.23880657e-01,\n",
       "        6.41116128e-01, -8.01404674e-01, -1.08432474e+00, -1.74192084e+00,\n",
       "        6.22732305e-02, -1.43576024e+00, -3.97793119e-01, -1.50201590e+00,\n",
       "       -1.07982225e-01, -4.88934045e-01, -3.88988078e-01,  1.00002703e+00,\n",
       "       -3.35366905e-02, -8.80050263e-01, -9.03073488e-01,  3.40421146e-01,\n",
       "       -6.85148620e-01, -8.37962258e-01, -1.01035105e+00, -1.07816730e+00,\n",
       "        8.06500431e-01,  1.03831619e+00,  1.11071090e-01, -1.21559695e+00,\n",
       "        7.76702958e-01,  1.14321991e+00, -1.42659090e+00,  5.36411797e-01,\n",
       "       -1.34564448e+00, -9.25891847e-01,  1.53789173e+00, -7.44500615e-02,\n",
       "        2.15126589e-01, -1.43290830e+00, -1.27217775e+00,  1.04397753e-01,\n",
       "        1.52352574e-01,  6.16485531e-01,  8.15145482e-02,  6.59023488e-01,\n",
       "       -5.95341260e-01,  9.76528953e-02, -7.12859810e-01, -1.97343135e-01,\n",
       "       -1.49100814e-01,  1.16530607e-01, -1.52267984e-01, -5.27903898e-01,\n",
       "       -5.75439621e-01, -9.16741720e-01, -1.30956990e-01, -6.08024413e-01,\n",
       "       -1.20501925e+00, -4.03143140e-01,  1.15083349e+00, -7.23099814e-01,\n",
       "       -4.41732657e-01, -1.74768753e-01, -7.54320944e-01,  1.30940629e-01,\n",
       "        3.24519181e-02,  2.47398326e-01, -3.82452484e-01,  9.46954298e-01,\n",
       "       -2.64115166e-01,  2.46551502e-01, -4.25649931e-01, -1.16402934e+00,\n",
       "        1.01002575e-01, -7.80083374e-01, -5.94724585e-02, -6.52957689e-01,\n",
       "        2.32112957e-01, -1.14275188e+00,  1.51214271e-01,  6.75290075e-01,\n",
       "        7.63321400e-01, -1.87581491e-01,  3.32571393e-01,  5.80262313e-01])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU,Embedding,Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          6000300   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100, 256)          427776    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               147840    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 6,576,303\n",
      "Trainable params: 6,576,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=sequence_length,\n",
    "                    trainable=True))\n",
    "model.add(GRU(256,activation=\"tanh\",return_sequences=True))\n",
    "model.add(GRU(128,activation=\"tanh\"))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 318087 samples, validate on 35344 samples\n",
      "Epoch 1/5\n",
      "318087/318087 [==============================] - 3807s 12ms/step - loss: 0.9262 - acc: 0.5487 - val_loss: 0.8597 - val_acc: 0.5967\n",
      "Epoch 2/5\n",
      "318087/318087 [==============================] - 3013s 9ms/step - loss: 0.7993 - acc: 0.6379 - val_loss: 0.8304 - val_acc: 0.6188\n",
      "Epoch 3/5\n",
      "318087/318087 [==============================] - 2766s 9ms/step - loss: 0.6976 - acc: 0.6960 - val_loss: 0.8623 - val_acc: 0.6121\n",
      "Epoch 4/5\n",
      "318087/318087 [==============================] - 3452s 11ms/step - loss: 0.5822 - acc: 0.7558 - val_loss: 0.9459 - val_acc: 0.6017\n",
      "Epoch 5/5\n",
      "318087/318087 [==============================] - 3286s 10ms/step - loss: 0.4570 - acc: 0.8157 - val_loss: 1.0886 - val_acc: 0.5852\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=1, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('baseline_model.h5') \n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline = load_model('baseline_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_baseline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = [np.argmax(predictions[i]) for i in range(len(predictions))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [np.argmax(y_test[i]) for i in range(len(y_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5853174097934863"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(true_labels,pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "match =[]\n",
    "mismatch=[]\n",
    "for i in range(len(true_labels)):\n",
    "    if true_labels[i]!=pred_list[i]:\n",
    "        mismatch.append(i)\n",
    "    else:\n",
    "        match.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16285"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22986"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           2,  5733,   111,  1047,  3320,  4537,     4, 12114,     2,\n",
       "        5733,    15,   756,  2068,    14,     2, 12114,     3,    22,\n",
       "         475], dtype=int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_X_test = tokenizer.sequences_to_texts(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_text=[]\n",
    "for i in mismatch:\n",
    "    mismatch_text.append(text_X_test[i])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_text = [mismatch_text[i].replace('<unw>',\"\") for i in range(len(mismatch_text))]\n",
    "mismatch_text = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                 the pump also offers superb ease of squeeze the pump was designed specifically for the squeeze to be easy'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatch_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "fmri = sns.load_dataset(\"fmri\")\n",
    "ax = sns.lineplot(x=\"timepoint\", y=\"signal\", data=fmri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
