@inproceedings{Nickel2017PoincarEF,
  title={Poincar{\'e} Embeddings for Learning Hierarchical Representations},
  author={Maximilian Nickel and Douwe Kiela},
  booktitle={NIPS},
  year={2017}
}

@inproceedings{glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@book{ungar2005,
  title={Analytic hyperbolic geometry: Mathematical foundations and applications},
  author={Ungar, Abraham A},
  year={2005},
  publisher={World Scientific}
}

@inproceedings{skipthought,
  title={Skip-thought vectors},
  author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Advances in neural information processing systems},
  pages={3294--3302},
  year={2015}
}

@inproceedings{hyperbolicqa,
  title={Hyperbolic representation learning for fast and efficient neural question answering},
  author={Tay, Yi and Tuan, Luu Anh and Hui, Siu Cheung},
  booktitle={Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
  pages={583--591},
  year={2018},
  organization={ACM}
}

@article{dhingra2018embedding,
  title={Embedding text in hyperbolic spaces},
  author={Dhingra, Bhuwan and Shallue, Christopher J and Norouzi, Mohammad and Dai, Andrew M and Dahl, George E},
  journal={arXiv preprint arXiv:1806.04313},
  year={2018}
}

@article{poincareglove,
  author    = {Alexandru Tifrea and
               Gary B{\'{e}}cigneul and
               Octavian{-}Eugen Ganea},
  title     = {Poincar{\'{e}} GloVe: Hyperbolic Word Embeddings},
  journal   = {CoRR},
  volume    = {abs/1810.06546},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.06546},
  archivePrefix = {arXiv},
  eprint    = {1810.06546},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-06546},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@incollection{hyperbolicnn,
title = {Hyperbolic Neural Networks},
author = {Ganea, Octavian and Becigneul, Gary and Hofmann, Thomas},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {5345--5355},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7780-hyperbolic-neural-networks.pdf}
}
@article{Miller95wordnet,
    author = {George A. Miller},
    title = {WordNet: A Lexical Database for English},
    journal = {COMMUNICATIONS OF THE ACM},
    year = {1995},
    volume = {38},
    pages = {39--41}
}
@article{Bahdanau2015,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:Users/dhruv/Library/Application Support/Mendeley Desktop/Downloaded/Bahdanau, Cho, Bengio - 2015 - Neural Machine Translation by Jointly Learning to Align and Translate(2).pdf:pdf},
isbn = {0147-006X (Print) 0147-006X (Linking)},
issn = {0147-006X},
journal = {Annual Review of Neuroscience},
keywords = {Neural machine translation is a recently proposed,Unlike the traditional statistical machine transla,a source sentence into a fixed-length vector from,and propose to extend this by allowing a model to,bottleneck in improving the performance of this ba,for parts of a source sentence that are relevant t,having to form these parts as a hard segment expli,machine translation often belong to a family of en,maximize the translation performance. The models p,phrase-based system on the task of English-to-Fren,qualitative analysis reveals that the (soft-)align,the neural machine,translation aims at building a single neural netwo,translation. In this paper,we achieve a translation performance comparable to,we conjecture that the use of a fixed-length vecto,well with our intuition,without},
number = {1},
pages = {105--131},
pmid = {14527267},
title = {{NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE}},
url = {https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/071b16f25117fb6133480c6259227d54fc2a5ea0 http://www.annualreviews.org/doi/abs/10.1146/annurev.neuro.26.041002.131047},
volume = {26},
year = {2015}
}
